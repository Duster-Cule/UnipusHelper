## 对于使用Ollama作为LLM平台的警告

---

在测试的过程中，我们发现，在家用电脑上运行的本地LLM效果皆不好。由于内存和算力不足，表现良好的LLM不足以在家用电脑上运行。我们测试了诸如openchat，llama3.3, deepseek-r1等开源模型，在12代i7-12700KF+RTX4060+32G RAM上作答试题（若下文没有说明，则测试条件均为此），选择题均不能保证100%正确率。

有趣的事情是，那些做错的题目，如果单独交给LLM生成答案，它就又能做对了，反而在整段文本中就会给出错误的答案。这可能和大文本的底层逻辑有关，这里不多赘述，希望能有高手解答。

对于推理我们测试了1.5b、7b、14b的deepseek-r1推理模型，在推理模型下deepseek-r1:14b能够达到可观水准，但是输出速度极慢，大部分落在2-4分钟区间，因此在程序中，对于ollama给予的回答时限为5分钟，而正常平台的API使用2分钟。

deepseek-r1:14b的可观水准指：选择题正确率90%左右，预计综合评分60-75分左右，可以超越绝大部分非推理模型（同参数下）

所以推荐LLM配置为：在算力平台上开启的ollama，非推理模型70b+，推理模型32b+。总的来说，在预算范围下越高越好。如果您现在的配置不足以达到此要求，我们强烈建议您改用LLM在线平台。